{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONUS404_pr-bin_preprocessor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File name: CONUS404_pr-changes.ipynb\\n    Author: Andreas Prein\\n    E-mail: prein@ucar.edu\\n    Date created: 19.05.2022\\n    Date last modified: 19.05.2022\\n\\n    ############################################################## \\n    Purpos:\\n\\n    - Rean in hourly precipitation data from CONUS404 \\n    - Save the data at lower precission to make it easier accessible\\n    - Calculate changes in the hourly precipitation distribution \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''File name: CONUS404_pr-changes.ipynb\n",
    "    Author: Andreas Prein\n",
    "    E-mail: prein@ucar.edu\n",
    "    Date created: 19.05.2022\n",
    "    Date last modified: 19.05.2022\n",
    "\n",
    "    ############################################################## \n",
    "    Purpos:\n",
    "\n",
    "    - Rean in hourly precipitation data from CONUS404 \n",
    "    - Save the data at lower precission to make it easier accessible\n",
    "    - Calculate changes in the hourly precipitation distribution \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import os\n",
    "from pdb import set_trace as stop\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import label\n",
    "from matplotlib import cm\n",
    "from scipy import ndimage\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from calendar import monthrange\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys \n",
    "import shapefile as shp\n",
    "import matplotlib.path as mplPath\n",
    "from scipy.stats import norm\n",
    "import matplotlib.gridspec as gridspec\n",
    "# from mpl_toolkits.basemap import Basemap, cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pylab import *\n",
    "import string\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import shapefile\n",
    "from calendar import monthrange\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry as sgeom\n",
    "from matplotlib.colors import LogNorm\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as cf\n",
    "\n",
    "from wrf import (to_np, getvar, smooth2d, get_cartopy, cartopy_xlim,\n",
    "                 cartopy_ylim, latlon_coords)\n",
    "\n",
    "def read_shapefile(sf):\n",
    "    \"\"\"\n",
    "    Read a shapefile into a Pandas dataframe with a 'coords' \n",
    "    column holding the geometry information. This uses the pyshp\n",
    "    package\n",
    "    \"\"\"\n",
    "    fields = [x[0] for x in sf.fields][1:]\n",
    "    records = sf.records()\n",
    "    shps = [s.points for s in sf.shapes()]\n",
    "    df = pd.DataFrame(columns=fields, data=records)\n",
    "    df = df.assign(coords=shps)\n",
    "    return df\n",
    "\n",
    "#### speed up interpolation\n",
    "import scipy.interpolate as spint\n",
    "import scipy.spatial.qhull as qhull\n",
    "import numpy as np\n",
    "\n",
    "def interp_weights(xy, uv,d=2):\n",
    "    tri = qhull.Delaunay(xy)\n",
    "    simplex = tri.find_simplex(uv)\n",
    "    vertices = np.take(tri.simplices, simplex, axis=0)\n",
    "    temp = np.take(tri.transform, simplex, axis=0)\n",
    "    delta = uv - temp[:, d]\n",
    "    bary = np.einsum('njk,nk->nj', temp[:, :d, :], delta)\n",
    "    return vertices, np.hstack((bary, 1 - bary.sum(axis=1, keepdims=True)))\n",
    "\n",
    "def interpolate(values, vtx, wts):\n",
    "    return np.einsum('nj,nj->n', np.take(values, vtx), wts)\n",
    "\n",
    "def deiscretice_timeseries(DATA,\n",
    "                          bucked_size):\n",
    "#     Discrete_timeseries = np.copy(DATA); Discrete_timeseries[:] = np.nan\n",
    "#     for tt in range(len(DATA)):\n",
    "#         if ~np.isnan(DATA[tt]) == True:   \n",
    "#             INT, REST = divmod(DATA[tt], bucked_size)\n",
    "#             Discrete_timeseries[tt] = INT * bucked_size\n",
    "#             if tt != len(DATA)-1:\n",
    "#                 DATA[tt+1] = DATA[tt+1]+REST\n",
    "#     return Discrete_timeseries\n",
    "\n",
    "    if len(DATA.shape) == 1:\n",
    "        # make data 2D\n",
    "        DATA = DATA[:,None]\n",
    "    Discrete_timeseries = np.copy(DATA); Discrete_timeseries[:] = np.nan\n",
    "    for tt in tqdm(range(DATA.shape[0])):\n",
    "        INT, REST = np.apply_along_axis(np.divmod, 0, DATA[tt,:], bucked_size)\n",
    "        FIN = ~np.isnan(INT)\n",
    "        Discrete_timeseries[tt,:] = INT * bucked_size\n",
    "        if tt != len(DATA)-1:\n",
    "            DATA[tt+1,FIN] = DATA[tt+1,FIN]+REST[FIN]\n",
    "    return Discrete_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                READ CONUS404 CONSTANT FIELDS\n",
    "sLon='XLONG'\n",
    "sLat='XLAT'\n",
    "sOro='HGT'\n",
    "sLSM='LANDMASK'\n",
    "sPlotDir = ''\n",
    "GEO_EM_D1 = '/glade/campaign/ncar/USGS_Water/CONUS404/wrfconstants_d01_1979-10-01_00:00:00.nc4'\n",
    "\n",
    "ncid=Dataset(GEO_EM_D1, mode='r') # open the netcdf\n",
    "Lon=np.squeeze(ncid.variables[sLon][:])\n",
    "Lat=np.squeeze(ncid.variables[sLat][:])\n",
    "Height4=np.squeeze(ncid.variables[sOro][:])\n",
    "LSM=np.squeeze(ncid.variables[sLSM][:])\n",
    "ncid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFolder = '/glade/campaign/mmm/c3we/prein/CONUS404/data/MonthlyData/'\n",
    "SaveFolder = '/glade/campaign/mmm/c3we/prein/CONUS404/data/CONUS404_processed_data/'\n",
    "\n",
    "StartDay = datetime.datetime(1981, 1, 1, 0)\n",
    "StopDay = datetime.datetime(1981, 12, 31, 23)\n",
    "TimeHH = pd.date_range(StartDay, end=StopDay, freq='1h')\n",
    "TimeMM = pd.date_range(StartDay, end=StopDay, freq='M')\n",
    "Years = np.unique(TimeMM.year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ HOURLY CONUS404 PRECIPITATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480/480 [11:48:35<00:00, 88.57s/it]   \n"
     ]
    }
   ],
   "source": [
    "CONUS404_hourly_pr = np.zeros((len(TimeHH), Lon.shape[0], Lon.shape[1]), dtype=np.float16)\n",
    "for mm in tqdm(range(len(TimeMM))):\n",
    "    YYYY = TimeMM[mm].year\n",
    "    MM = TimeMM[mm].month\n",
    "    rgiHours = (TimeHH.year == YYYY) & (TimeHH.month == MM)\n",
    "    File_act = DataFolder + 'PREC_ACC_NC_'+str(YYYY)+str(MM).zfill(2)+'_CONUS404.nc'\n",
    "    ncid=Dataset(File_act, mode='r') # open the netcdf\n",
    "    CONUS404_hourly_pr[rgiHours,:,:] = np.array(np.squeeze(ncid.variables['PREC_ACC_NC'][:]), dtype=np.float16)\n",
    "    ncid.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate annual CDFs (sort of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work on 1980\n",
      "work on 1981\n",
      "    annual\n"
     ]
    }
   ],
   "source": [
    "Subsample = 10\n",
    "Seasons = ['annual','DJF','MAM','JJA','SON']\n",
    "rgiSeasons = [range(1,13,1),\n",
    "                 [1,2,12],\n",
    "                 [3,4,5],\n",
    "                 [6,7,8],\n",
    "                 [9,10,11]]\n",
    "for yy in range(len(Years)):\n",
    "    print('work on '+str(Years[yy]))\n",
    "    for se in range(len(Seasons)):\n",
    "        rgiTime = (TimeHH.year == Years[yy]) & np.isin(TimeHH.month, rgiSeasons[se])\n",
    "        TimeAct = TimeHH[rgiTime]\n",
    "        SaveFile = SaveFolder+'CONUS404_bined_pr/PREC_ACC_NC_'+str(TimeAct[0].year)+'_'+Seasons[se]+'.nc'\n",
    "        if os.path.exists(SaveFile) == False:\n",
    "            print('    '+Seasons[se])\n",
    "            \n",
    "            Data_year = np.copy(CONUS404_hourly_pr[rgiTime,:,:])\n",
    "            sort_data = np.sort(Data_year, axis=0)\n",
    "            whole_div = int(np.sum(rgiTime)/Subsample)*Subsample\n",
    "            binned_pr = np.mean(np.reshape(sort_data[-whole_div:,:,:], (int(whole_div/Subsample), Subsample, sort_data.shape[1], sort_data.shape[2])), axis=1)\n",
    "        \n",
    "            # ---------------------------------------------\n",
    "            # write data to netcdf\n",
    "            da = xr.DataArray(\n",
    "            data=np.array(binned_pr, dtype=np.float32),\n",
    "            dims=[\"percentile\", \"x\", \"y\"],\n",
    "            coords=dict(\n",
    "                    percentile = np.linspace(0,100,binned_pr.shape[0]),\n",
    "                    lon=([\"x\", \"y\"], Lon),\n",
    "                    lat=([\"x\", \"y\"], Lat),\n",
    "                ),\n",
    "                attrs=dict(\n",
    "                    description=\"sorted precipitation bin averages\",\n",
    "                    units=\"mm h-1\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            ds = da.to_dataset(name='precipitation')\n",
    "            ds.to_netcdf(path=SaveFile, mode='w')\n",
    "            \n",
    "            # ---------------------------------------------\n",
    "            # Also, save the annual/seasonal maximum\n",
    "            SaveFile = SaveFolder+'CONUS404_an-seas_max-PR/Max_PREC_ACC_NC_'+str(TimeAct[0].year)+'_'+Seasons[se]+'.nc'\n",
    "            da = xr.DataArray(\n",
    "            data=np.array(sort_data[-1,:,:], dtype=np.float32),\n",
    "            dims=[\"x\", \"y\"],\n",
    "            coords=dict(\n",
    "                    lon=([\"x\", \"y\"], Lon),\n",
    "                    lat=([\"x\", \"y\"], Lat),\n",
    "                ),\n",
    "                attrs=dict(\n",
    "                    description=\"maximum precipitation\",\n",
    "                    units=\"mm h-1\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            ds = da.to_dataset(name='max_precipitation')\n",
    "            ds.to_netcdf(path=SaveFile, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
