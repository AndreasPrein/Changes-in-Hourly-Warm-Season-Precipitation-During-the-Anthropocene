{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [31:00<00:00, 155.05s/it]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "# # Scaling_Changes.ipynb\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "'''File name: Scaling_Changes.ipynb\n",
    "    Author: Andreas Prein\n",
    "    E-mail: prein@ucar.edu\n",
    "    Date created: 26.05.2022\n",
    "    Date last modified: 26.05.2022\n",
    "\n",
    "    ############################################################## \n",
    "    Purpos:\n",
    "\n",
    "    - Rean in hourly precipitation data from CONUS404 \n",
    "    - Read in hourly dewpoint temperature data from CONUS404\n",
    "    - Calculate scaling diagrams by year and region \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import os\n",
    "from pdb import set_trace as stop\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import label\n",
    "from matplotlib import cm\n",
    "from scipy import ndimage\n",
    "import random\n",
    "import scipy\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from calendar import monthrange\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys \n",
    "import shapefile as shp\n",
    "import matplotlib.path as mplPath\n",
    "from scipy.stats import norm\n",
    "import matplotlib.gridspec as gridspec\n",
    "# from mpl_toolkits.basemap import Basemap, cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pylab import *\n",
    "import string\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import shapefile\n",
    "from calendar import monthrange\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry as sgeom\n",
    "from matplotlib.colors import LogNorm\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import cartopy.feature as cf\n",
    "\n",
    "from wrf import (to_np, getvar, smooth2d, get_cartopy, cartopy_xlim,\n",
    "                 cartopy_ylim, latlon_coords)\n",
    "\n",
    "def read_shapefile(sf):\n",
    "    \"\"\"\n",
    "    Read a shapefile into a Pandas dataframe with a 'coords' \n",
    "    column holding the geometry information. This uses the pyshp\n",
    "    package\n",
    "    \"\"\"\n",
    "    fields = [x[0] for x in sf.fields][1:]\n",
    "    records = sf.records()\n",
    "    shps = [s.points for s in sf.shapes()]\n",
    "    df = pd.DataFrame(columns=fields, data=records)\n",
    "    df = df.assign(coords=shps)\n",
    "    return df\n",
    "\n",
    "#### speed up interpolation\n",
    "import scipy.interpolate as spint\n",
    "import scipy.spatial.qhull as qhull\n",
    "import numpy as np\n",
    "\n",
    "def interp_weights(xy, uv,d=2):\n",
    "    tri = qhull.Delaunay(xy)\n",
    "    simplex = tri.find_simplex(uv)\n",
    "    vertices = np.take(tri.simplices, simplex, axis=0)\n",
    "    temp = np.take(tri.transform, simplex, axis=0)\n",
    "    delta = uv - temp[:, d]\n",
    "    bary = np.einsum('njk,nk->nj', temp[:, :d, :], delta)\n",
    "    return vertices, np.hstack((bary, 1 - bary.sum(axis=1, keepdims=True)))\n",
    "\n",
    "def interpolate(values, vtx, wts):\n",
    "    return np.einsum('nj,nj->n', np.take(values, vtx), wts)\n",
    "\n",
    "def deiscretice_timeseries(DATA,\n",
    "                          bucked_size):\n",
    "#     Discrete_timeseries = np.copy(DATA); Discrete_timeseries[:] = np.nan\n",
    "#     for tt in range(len(DATA)):\n",
    "#         if ~np.isnan(DATA[tt]) == True:   \n",
    "#             INT, REST = divmod(DATA[tt], bucked_size)\n",
    "#             Discrete_timeseries[tt] = INT * bucked_size\n",
    "#             if tt != len(DATA)-1:\n",
    "#                 DATA[tt+1] = DATA[tt+1]+REST\n",
    "#     return Discrete_timeseries\n",
    "\n",
    "    if len(DATA.shape) == 1:\n",
    "        # make data 2D\n",
    "        DATA = DATA[:,None]\n",
    "    Discrete_timeseries = np.copy(DATA); Discrete_timeseries[:] = np.nan\n",
    "    for tt in tqdm(range(DATA.shape[0])):\n",
    "        INT, REST = np.apply_along_axis(np.divmod, 0, DATA[tt,:], bucked_size)\n",
    "        FIN = ~np.isnan(INT)\n",
    "        Discrete_timeseries[tt,:] = INT * bucked_size\n",
    "        if tt != len(DATA)-1:\n",
    "            DATA[tt+1,FIN] = DATA[tt+1,FIN]+REST[FIN]\n",
    "    return Discrete_timeseries\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# ================================\n",
    "# BUKOFSKY REGION\n",
    "# Add the subregions\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "REGIONS = [ 'Appalachia.shp',\n",
    "            'CPlains.shp',\n",
    "            'DeepSouth.shp',\n",
    "            'GreatBasin.shp',\n",
    "            'GreatLakes.shp',\n",
    "            'Mezquital.shp',\n",
    "            'MidAtlantic.shp',\n",
    "            'NorthAtlantic.shp',\n",
    "            'NPlains.shp',\n",
    "            'NRockies.shp',\n",
    "            'PacificNW.shp',\n",
    "            'PacificSW.shp',\n",
    "            'Prairie.shp',\n",
    "            'Southeast.shp',\n",
    "            'Southwest.shp',\n",
    "            'SPlains.shp',\n",
    "            'SRockies.shp']\n",
    "\n",
    "REGIONS_names = [ 'Appalachia',\n",
    "            'Central Plains',\n",
    "            'Deep South',\n",
    "            'Great Basin',\n",
    "            'Great Lakes',\n",
    "            'Mezquital',\n",
    "            'Mid-Atlantic',\n",
    "            'North-Atlantic',\n",
    "            'Northern Plains',\n",
    "            'Northern Rockies',\n",
    "            'Pacific Northwest',\n",
    "            'Pacific Southwest',\n",
    "            'Prairie',\n",
    "            'Southeast',\n",
    "            'Southwest',\n",
    "            'Southern Plains',\n",
    "            'Southern Rockies']\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#                READ CONUS404 CONSTANT FIELDS\n",
    "sLon='XLONG'\n",
    "sLat='XLAT'\n",
    "sOro='HGT'\n",
    "sLSM='LANDMASK'\n",
    "sPlotDir = ''\n",
    "GEO_EM_D1 = '/glade/campaign/ncar/USGS_Water/CONUS404/wrfconstants_d01_1979-10-01_00:00:00.nc4'\n",
    "\n",
    "ncid=Dataset(GEO_EM_D1, mode='r') # open the netcdf\n",
    "Lon=np.squeeze(ncid.variables[sLon][:])\n",
    "Lat=np.squeeze(ncid.variables[sLat][:])\n",
    "Height4=np.squeeze(ncid.variables[sOro][:])\n",
    "LSM=np.squeeze(ncid.variables[sLSM][:])\n",
    "ncid.close()\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "DataFolder = '/glade/campaign/mmm/c3we/prein/CONUS404/data/MonthlyData/'\n",
    "SaveFolder = '/glade/campaign/mmm/c3we/prein/CONUS404/data/CONUS404_processed_data/'\n",
    "\n",
    "StartDay = datetime.datetime(1980, 1, 1, 0)\n",
    "StopDay = datetime.datetime(2019, 12, 31, 23)\n",
    "TimeHH = pd.date_range(StartDay, end=StopDay, freq='1h')\n",
    "TimeMM = pd.date_range(StartDay, end=StopDay, freq='M')\n",
    "Years = np.unique(TimeMM.year)\n",
    "YYYY = 2000 # int(sys.argv[1])\n",
    "\n",
    "Seasons = ['annual','DJF','MAM','JJA','SON']\n",
    "rgiSeasons = [range(1,13,1),\n",
    "                 [1,2,12],\n",
    "                 [3,4,5],\n",
    "                 [6,7,8],\n",
    "                 [9,10,11]]\n",
    "\n",
    "dry_threshold = 0.1 # mm/h\n",
    "lag_hours = 2 # hours before the pr event that dT is taken\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "MaskFile = '../CONUS404_pr-changes/Burkofski_Regions_CONUS404.npz'\n",
    "if os.path.exists(MaskFile) == False:\n",
    "    rgiStatsInBasins = []\n",
    "    MaskStations = np.zeros((Lon.shape[0],Lon.shape[1])); MaskStations[:] = np.nan\n",
    "    rgrGridCells=[(Lon.flatten()[ii],Lat.flatten()[ii]) for ii in range(len(Lon.flatten()))]\n",
    "    for re in tqdm(range(len(REGIONS))):\n",
    "        data = gpd.read_file('/glade/u/home/prein/papers/2021_Hist-Ext-PR-Changes/shapefiles/Bukovski-Regions/'+REGIONS[re])\n",
    "        Coordinates = data['geometry']\n",
    "        for sf in range(len(data)):\n",
    "            TEST = np.array(Coordinates[sf].exterior.coords.xy)\n",
    "            ctr=TEST.T\n",
    "            grPRregion=mplPath.Path(ctr)\n",
    "            TMP=np.array(grPRregion.contains_points(rgrGridCells))\n",
    "        TMP = np.reshape(TMP, (Lon.shape[0], Lon.shape[1]))\n",
    "        MaskStations[TMP==1] = re+1\n",
    "    #     MaskStations = np.append(MaskStations,[re+1]*len(iStationSelect))\n",
    "    MaskStations = MaskStations.astype('int')\n",
    "    \n",
    "    np.savez(MaskFile,\n",
    "            MaskStations = MaskStations,\n",
    "            Lon = Lon,\n",
    "            Lat = Lat)\n",
    "else:\n",
    "    DATA = np.load(MaskFile)\n",
    "    MaskStations = DATA['MaskStations']\n",
    "    Lon = DATA['Lon']\n",
    "    Lat = DATA['Lat']\n",
    "MaskStations[MaskStations < 0] = 0\n",
    "\n",
    "Region_indices = ndimage.find_objects(MaskStations)\n",
    "# Region_indices = ndimage.find_objects(MaskStations)\n",
    "\n",
    "\n",
    "# ### Read CONUS404 data and calculate scalling curves year by year\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "bins_dT = np.arange(-30,35,0.1)\n",
    "pr_perc = (99,99.5,99.9,99.99)\n",
    "\n",
    "\n",
    "# In[140]:\n",
    "\n",
    "\n",
    "# for yy in [0]: #tqdm(range(len(Years))):\n",
    "#     YYYY = Years[yy]\n",
    "rgiHours = (TimeHH.year == YYYY)\n",
    "timeHH_yy = TimeHH[rgiHours]\n",
    "prec_yy = np.array(np.zeros((len(timeHH_yy), Lon.shape[0], Lon.shape[1])), dtype=np.float32); prec_yy[:] = np.nan\n",
    "dT_yy = np.copy(prec_yy)\n",
    "# Bin the data\n",
    "binned_pr = np.zeros((len(bins_dT), len(pr_perc), Lon.shape[0] * Lon.shape[1])); binned_pr[:] = np.nan\n",
    "\n",
    "# Bin according to DT\n",
    "NN = 100\n",
    "bins = np.array([0]+ [np.exp(np.log(0.005) + (ii* ((np.log(160)-np.log(0.005))**2/(NN)) )**0.5 ) for ii in range(NN)])\n",
    "bins_cent = (bins[1:]+bins[:-1])/2\n",
    "dT_perc = (99.9, 99, 50, 1, 0.1)\n",
    "binned_dT = np.zeros((len(bins_cent), len(dT_perc), len(REGIONS_names))); binned_dT[:] = np.nan\n",
    "\n",
    "\n",
    "for mm in tqdm(range(12)):\n",
    "    MM = mm+1\n",
    "    time_mm_in_yy = timeHH_yy.month == MM\n",
    "\n",
    "    # read precipitation\n",
    "    File_act = DataFolder + 'PREC_ACC_NC_'+str(YYYY)+str(MM).zfill(2)+'_CONUS404.nc'\n",
    "    ncid=Dataset(File_act, mode='r') # open the netcdf\n",
    "    prec_yy[time_mm_in_yy,:,:] = np.array(np.squeeze(ncid.variables['PREC_ACC_NC'][:]), dtype=np.float32)\n",
    "    ncid.close()\n",
    "\n",
    "    # read 2m dewpoint temperature\n",
    "    File_act = DataFolder + 'TD2_'+str(YYYY)+str(MM).zfill(2)+'_CONUS404.nc'\n",
    "    ncid=Dataset(File_act, mode='r') # open the netcdf\n",
    "    dT_yy[time_mm_in_yy,:,:] = np.array(np.squeeze(ncid.variables['TD2'][:]), dtype=np.float32)-273.15\n",
    "    ncid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_yy = np.reshape(prec_yy, (prec_yy.shape[0], Lon.shape[0] * Lon.shape[1]))\n",
    "dT_yy   = np.reshape(dT_yy, (prec_yy.shape[0], Lon.shape[0] * Lon.shape[1]))\n",
    "\n",
    "rgi_max_pr = np.nanargmax(prec_yy, axis=0)\n",
    "m,n = prec_yy.shape[:2]\n",
    "max_pr = prec_yy[rgi_max_pr,np.arange(n)]\n",
    "max_pr = np.reshape(max_pr, (Lon.shape[0],Lon.shape[1]))\n",
    "\n",
    "\n",
    "rgi_max_dT = np.copy(rgi_max_pr)\n",
    "rgi_max_dT = rgi_max_dT - lag_hours\n",
    "rgi_max_dT[rgi_max_dT < 0]= 0\n",
    "max_dt = dT_yy[rgi_max_dT,np.arange(n)]\n",
    "max_dt = np.reshape(max_dt, (Lon.shape[0],Lon.shape[1]))\n",
    "\n",
    "np.savez(SaveFolder+'pr_vs_dT/'+str(YYYY)+'_pr-vs-dT_gridcells.npz',\n",
    "        dry_threshold = dry_threshold,\n",
    "        lag_hours = lag_hours,\n",
    "        max_pr = max_pr,\n",
    "        max_dt = max_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
